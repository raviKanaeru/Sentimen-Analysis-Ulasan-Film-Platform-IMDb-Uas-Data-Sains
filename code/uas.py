# -*- coding: utf-8 -*-
"""uas.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1B2BSAnFHiKeYv6jkac5sJQnbH2snTj_P
"""

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('./kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))


import tensorflow as tf
# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All"
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

import seaborn as sns
import matplotlib.pyplot as plt
from collections import Counter
import re
from sklearn.feature_extraction.text import CountVectorizer
from wordcloud import WordCloud
import spacy
import nltk
from nltk.stem import PorterStemmer
from nltk.corpus import stopwords

df=pd.read_csv('./kaggle/input/imdb-dataset-sentiment-analysis-in-csv-format/Test.csv')
df.head()

df.isnull().sum()

sentiment_count = Counter(df['label'])
print(sentiment_count)

sns.countplot(x='label', data=df)
plt.title('Distribution of Sentiments')
plt.show()

df['text_length'] = df['text'].apply(len)

sns.histplot(x='text_length', data=df)
##plt.xlim(0, 2000)
plt.title('Distribution of Text Lengths')
plt.show()

sns.histplot(x='text_length', data=df)
plt.xlim(0, 2000)
plt.title('Distribution of Text Lengths')
plt.show()

xmin = 0
xmax = 1500

# Boolean indexing to split the DataFrame
mask = (df['text'].apply(len) >= xmin) & (df['text'].apply(len) <= xmax)
df_range = df.loc[mask].copy()

df_range['text_length'] = df_range['text'].apply(len)

sns.histplot(x='text_length', data=df_range)
plt.title('Distribution of Text Lengths')
plt.show()

sns.countplot(x='label', data=df_range)
plt.title('Distribution of Sentiments')
plt.show()

sns.histplot(x='text_length', data=df_range, hue='label', multiple='stack')
plt.title('Distribution of Text Lengths by Sentiment')
plt.show()

df_range['text'] = df_range['text'].apply(lambda x: re.sub(r'\W', ' ', str(x))) # remove non-alphanumeric characters

df_range['text'] = df_range['text'].apply(lambda x: re.sub(r'\s+[a-zA-Z]\s+', ' ', x)) # remove single character words

df_range['text'] = df_range['text'].apply(lambda x: re.sub(r'\^[a-zA-Z]\s+', ' ', x)) # remove single characters at the beginning of text

import pandas as pd
import re

# before replace multiple spaces with single space
df_before = df_range['text'].copy()
df_range['text'] = df_range['text'].apply(lambda x: re.sub(r'\s+', ' ', x, flags=re.I)) # replace multiple spaces with single space

# after replace multiple spaces with single space
df_after = df_range['text'].copy()

# view table
print("before replace multiple spaces with single space:")
print(df_before)
print("\nafter replace multiple spaces with single space:")
print(df_after)

import pandas as pd
import re

# before convert to lowercase
df_before = df_range['text'].copy()
df_range['text'] = df_range['text'].apply(lambda x: x.lower()) # convert to lowercase

# after convert to lowercase
df_after = df_range.copy()

# Tampilkan tabel sebelum dan sesudah
print("before convert to lowercase:")
print(df_before)
print("\nafter convert to lowercase:")
print(df_after)

nlp = spacy.load('en_core_web_sm')
stopwords = nlp.Defaults.stop_words
df_range['text_processed'] = df_range['text'].apply(lambda x: ' '.join([token.text for token in nlp(x) if not token.is_stop and not token.is_punct]))

df_range.head()

word_count = df_range['text'].apply(lambda x: len(x.split()))
sns.histplot(word_count)
plt.title('Distribution of Word Counts')
plt.show()

df_range['word_count'] = word_count
df_range.groupby('word_count')['label'].mean().plot.hist()
#plt.title('Sentiment Distribution by Word Count')
plt.ylabel('word_count')
#plt.xticks(rotation=90)
plt.xlabel('Label')
plt.title('Sentiment vs word_count')
plt.savefig('Sentiment vs word_count')
plt.show()
plt.show()

positive_texts = df_range[df_range['label'] == 1]['text_processed']
positive_wordcloud = WordCloud(width=800, height=800, background_color='black', stopwords=set()).generate(' '.join(positive_texts))
plt.figure(figsize=(8, 8), facecolor=None)
plt.imshow(positive_wordcloud)
plt.axis('off')
plt.tight_layout(pad=0)
plt.title('Word Cloud for Positive Sentiment')
plt.show()

negative_texts = df_range[df_range['label'] == 0]['text_processed']
negative_wordcloud = WordCloud(width=800, height=800, background_color='black', stopwords=set()).generate(' '.join(negative_texts))
plt.figure(figsize=(8, 8), facecolor=None)
plt.imshow(negative_wordcloud)
plt.axis('off')
plt.tight_layout(pad=0)
plt.title('Word Cloud for Negative Sentiment')
plt.show()

from nltk.corpus import stopwords

nltk.download('stopwords')
ps = PorterStemmer()
df_range['text'] = df_range['text'].apply(lambda x: ' '.join([ps.stem(word) for word in x.split() if word not in set(stopwords.words('english'))]))
df_range['text_processed'] = df_range['text_processed'].apply(lambda x: ' '.join([ps.stem(word) for word in x.split() if word not in set(stopwords.words('english'))]))

#randomization
df_range = df_range.sample(frac=1).reset_index(drop=True)
df_range.head()

accuracy = {'TF-IDF': []}

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split

# Contoh data teks dan label
X = df_range['text']
y = df_range['label']

# Vektorisasi teks menggunakan TF-IDF
vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(X)

# Bagi data menjadi data pelatihan dan data uji
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)



# Support Vector Machine
svm = SVC(kernel='linear')
svm.fit(X_train, y_train)
svm_preds = svm.predict(X_test)
svm_acc = accuracy_score(y_test, svm_preds)
print(f"SVM Accuracy: {svm_acc}")

import warnings
warnings.filterwarnings('ignore')

from sklearn.model_selection import GridSearchCV
from sklearn.svm import SVC

train_size = int(len(df_range) * 0.8)
train_data = df_range[:train_size]
test_data = df_range[train_size:]

tfidf = TfidfVectorizer()
tfidf.fit(train_data['text'])
X_train = tfidf.transform(train_data['text'])
X_test = tfidf.transform(test_data['text'])

# Membuat model SVM
svm_model = SVC()

# Tentukan hyperparameters yang ingin di-tune
param_grid = {
    'C': [0.01, 0.1, 1, 10, 100],
    'kernel': ['linear', 'rbf', 'poly']
}

# Buat objek GridSearchCV untuk mencari kombinasi hyperparameter terbaik
grid_search = GridSearchCV(svm_model, param_grid, cv=5)
grid_search.fit(X_train, train_data['label'])

# Dapatkan kombinasi hyperparameter terbaik dari GridSearch
best_params = grid_search.best_params_

# Buat model SVM dengan kombinasi hyperparameter terbaik
svm_model = SVC(**best_params)
svm_model.fit(X_train, train_data['label'])

# Evaluasi akurasi pada data uji
svm_acc = svm_model.score(X_test, test_data['label'])
accuracy['TF-IDF'].append(svm_acc)

print(f"SVM Accuracy: {svm_acc}")